{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "2393070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170701070956601</td>\n",
       "      <td>721C65A950C743ECA964282ED055DF5E</td>\n",
       "      <td>픽미가  구준엽 작품이라구?  대단하군 트렌드적중</td>\n",
       "      <td>[픽미, 구준엽, 작품, 구, ?, 대단하다, 트렌드, 적중]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170701070956601</td>\n",
       "      <td>A265E5185F4E476E930D459359B3904C</td>\n",
       "      <td>우정과 의리의 대명사 쿵따리</td>\n",
       "      <td>[우정, 의리, 대명사, 쿵, 따리]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170701070956601</td>\n",
       "      <td>BC4CC51D1CE845459F3C16245772E4C7</td>\n",
       "      <td>픽미 같은곳에서 벚꽃이 지면! 1년이 지난 지금도 사랑하는 곡들! 들을 때마다 소녀...</td>\n",
       "      <td>[픽미, 같다, 곳, 벚꽃, 지면, !, 1, 년, 지난, 지금, 사랑, 곡, 들다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170701070956601</td>\n",
       "      <td>5FBBAD1A12E34744A360630DB7CE321D</td>\n",
       "      <td>구준엽..정말 감탄나오는 의리있는 친구.. 강원래 사고나고 몇년이 지났건만 강원래를...</td>\n",
       "      <td>[구준엽, 정말, 감탄, 나오다, 의리, 있다, 친구, 강원래, 사다, 몇, 년, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170701070956601</td>\n",
       "      <td>1A607535BF024C098DBB37E1BE82E1CC</td>\n",
       "      <td>끝까지 친구 챙기는 디제이쿠 멋지다</td>\n",
       "      <td>[끝, 친구, 챙기다, 디제, 이쿠, 멋지다]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             news_id                        comment_id  \\\n",
       "0  20170701070956601  721C65A950C743ECA964282ED055DF5E   \n",
       "1  20170701070956601  A265E5185F4E476E930D459359B3904C   \n",
       "2  20170701070956601  BC4CC51D1CE845459F3C16245772E4C7   \n",
       "3  20170701070956601  5FBBAD1A12E34744A360630DB7CE321D   \n",
       "4  20170701070956601  1A607535BF024C098DBB37E1BE82E1CC   \n",
       "\n",
       "                                             comment  \\\n",
       "0                        픽미가  구준엽 작품이라구?  대단하군 트렌드적중   \n",
       "1                                    우정과 의리의 대명사 쿵따리   \n",
       "2  픽미 같은곳에서 벚꽃이 지면! 1년이 지난 지금도 사랑하는 곡들! 들을 때마다 소녀...   \n",
       "3  구준엽..정말 감탄나오는 의리있는 친구.. 강원래 사고나고 몇년이 지났건만 강원래를...   \n",
       "4                                끝까지 친구 챙기는 디제이쿠 멋지다   \n",
       "\n",
       "                                               token  \n",
       "0                 [픽미, 구준엽, 작품, 구, ?, 대단하다, 트렌드, 적중]  \n",
       "1                               [우정, 의리, 대명사, 쿵, 따리]  \n",
       "2  [픽미, 같다, 곳, 벚꽃, 지면, !, 1, 년, 지난, 지금, 사랑, 곡, 들다...  \n",
       "3  [구준엽, 정말, 감탄, 나오다, 의리, 있다, 친구, 강원래, 사다, 몇, 년, ...  \n",
       "4                          [끝, 친구, 챙기다, 디제, 이쿠, 멋지다]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "with open('D:\\\\WorkSpace\\\\뉴스 데이터\\\\df_comment0327.pickle', 'rb') as file:\n",
    "    df_comment = pickle.load(file)\n",
    "\n",
    "print(type(df_comment))\n",
    "print(len(df_comment))\n",
    "df_comment.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [픽미, 구준엽, 작품, 구, ?, 대단하다, 트렌드, 적중]\n",
       "1                                 [우정, 의리, 대명사, 쿵, 따리]\n",
       "2    [픽미, 같다, 곳, 벚꽃, 지면, !, 1, 년, 지난, 지금, 사랑, 곡, 들다...\n",
       "3    [구준엽, 정말, 감탄, 나오다, 의리, 있다, 친구, 강원래, 사다, 몇, 년, ...\n",
       "4                            [끝, 친구, 챙기다, 디제, 이쿠, 멋지다]\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data = df_comment.token\n",
    "token_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_to_int(token_data):\n",
    "    unique_comment_tokenized = [list(i) for i in set(tuple(i) for i in token_data)]\n",
    "    word_dic = {}\n",
    "\n",
    "    for words in unique_comment_tokenized:\n",
    "        for word in words:\n",
    "            if not(word in word_dic):\n",
    "                word_dic[word] = 0\n",
    "            word_dic[word] += 1\n",
    "\n",
    "    keys = sorted(word_dic.items(), key = lambda x:x[1], reverse = True)\n",
    "    for word, count in keys[:15]:\n",
    "        print(\"{0}({1}) \". format(word, count), end = \"\")\n",
    "    \n",
    "    # [] 없애주는 코드\n",
    "    from itertools import chain\n",
    "    words = set(chain(*unique_comment_tokenized))\n",
    "\n",
    "    # create mapping of unique chars to integers\n",
    "    word_to_int = dict((c,i) for i, c in enumerate(words))\n",
    "    n_vocab = len(word_to_int)\n",
    "        \n",
    "    print (\"\")\n",
    "    print (\"Total Vocab: \", n_vocab)\n",
    "    print (\"\")\n",
    "    comment_tokenized_int = []\n",
    "\n",
    "    for comment in token_data:\n",
    "        item_int = []\n",
    "        for item in comment:\n",
    "            item_int.append(word_to_int.get(item))\n",
    "        comment_tokenized_int.append(item_int)\n",
    "    \n",
    "    return comment_tokenized_int, n_vocab, word_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?(359287) 되다(277243) 있다(271479) 없다(226267) 아니다(207561) 안(193443) 보다(173535) 사람(151362) 같다(135933) 그렇다(129087) 못(125523) 들다(124497) 않다(119280) !(115791) 가다(110580) \n",
      "Total Vocab:  231398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_int, n_vocab, word_to_int = get_word_to_int(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary(par1):\n",
    "    import statistics\n",
    "\n",
    "    sum1 = []\n",
    "    for item in par1:\n",
    "        a = len(item)\n",
    "        sum1.append(a)\n",
    "\n",
    "    print (\"len: \", len(sum1))\n",
    "    print (\"max: \", max(sum1))\n",
    "    print (\"min: \", min(sum1))\n",
    "    print (\"sum: \", sum(sum1))\n",
    "    print (\"median: \", statistics.median(sum1))\n",
    "    print (\"mean: \", statistics.mean(sum1))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  231398\n",
      "max:  300\n",
      "min:  1\n",
      "sum:  739879\n",
      "median:  3.0\n",
      "mean:  3.1974304012999246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary(word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('수가동', 0), ('조사사해', 1), ('포텐셜', 2), ('추움', 3), ('한기수', 4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_int.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('D:\\\\WorkSpace\\\\뉴스 데이터\\\\word_index0327.pickle', 'wb') as file:    # hello.txt 파일을 바이너리 쓰기 모드(rb)로 열기\n",
    "#     pickle.dump(word_to_int, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
