{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:39:57.160036Z",
     "start_time": "2020-03-26T09:39:55.609184Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from util.text_preprocessing import load_data, tokenize_comment, count_comment\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:21:16.500973Z",
     "start_time": "2020-03-27T08:21:16.496983Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.normpath('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T07:06:20.478628Z",
     "start_time": "2020-03-21T07:06:20.469651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daum_comments_201707_entertain.txt',\n",
       " 'daum_comments_201707_non_entertain.txt',\n",
       " 'daum_comments_201708_entertain.txt',\n",
       " 'daum_comments_201708_non_entertain.txt',\n",
       " 'daum_comments_201709_entertain.txt',\n",
       " 'daum_comments_201709_non_entertain.txt',\n",
       " 'daum_comments_201710_entertain.txt',\n",
       " 'daum_comments_201710_non_entertain.txt',\n",
       " 'daum_comments_201711_entertain.txt',\n",
       " 'daum_comments_201711_non_entertain.txt',\n",
       " 'daum_comments_201712_entertain.txt',\n",
       " 'daum_comments_201712_non_entertain.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[path for path in os.listdir(os.path.join(DATA_PATH,\"raw_data\")) if path[-3:] == 'txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T07:06:21.089964Z",
     "start_time": "2020-03-21T07:06:21.084978Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_comments_df = pd.DataFrame()\n",
    "path_only_txt = [path for path in os.listdir(DATA_PATH) if path[-3:] == 'txt']\n",
    "\n",
    "for path in path_only_txt:\n",
    "    print(path)\n",
    "    data_df = load_data(os.path.join(DATA_PATH,path))\n",
    "    data_df = data_df[data_df.comment != \"\"]\n",
    "    total_comments_df = total_comments_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T07:06:25.423377Z",
     "start_time": "2020-03-21T07:06:23.032770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72985</th>\n",
       "      <td>20171225140102356</td>\n",
       "      <td>D3476B1E764C4FD4B7B8167B52521862</td>\n",
       "      <td>나도 그장면에서 폭풍눈물ㅠ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175574</th>\n",
       "      <td>20170713200956476</td>\n",
       "      <td>D6F56741C3C24C68A0918DA5A6F2B1DC</td>\n",
       "      <td>먹어보고 싶다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32144</th>\n",
       "      <td>20170703055742859</td>\n",
       "      <td>A1D2E0B94C0B4B5493699AABF7680C4F</td>\n",
       "      <td>XX년!.  패션쇼하러 갔냐? 하고...떠들어대던  좌빨들 어디들  갔냐? 우아는~...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61154</th>\n",
       "      <td>20170807150054502</td>\n",
       "      <td>4A62795E7A204100ABF07C32F267A97A</td>\n",
       "      <td>더럽게 많네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217932</th>\n",
       "      <td>20171129202704416</td>\n",
       "      <td>7EA376A64E394108A80EC601A7E78124</td>\n",
       "      <td>이것들도 적폐네</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  news_id                        comment_id  \\\n",
       "72985   20171225140102356  D3476B1E764C4FD4B7B8167B52521862   \n",
       "175574  20170713200956476  D6F56741C3C24C68A0918DA5A6F2B1DC   \n",
       "32144   20170703055742859  A1D2E0B94C0B4B5493699AABF7680C4F   \n",
       "61154   20170807150054502  4A62795E7A204100ABF07C32F267A97A   \n",
       "217932  20171129202704416  7EA376A64E394108A80EC601A7E78124   \n",
       "\n",
       "                                                  comment  \n",
       "72985                                      나도 그장면에서 폭풍눈물ㅠ  \n",
       "175574                                           먹어보고 싶다.  \n",
       "32144   XX년!.  패션쇼하러 갔냐? 하고...떠들어대던  좌빨들 어디들  갔냐? 우아는~...  \n",
       "61154                                              더럽게 많네  \n",
       "217932                                           이것들도 적폐네  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total_comments_df.to_pickle(os.path.join(DATA_PATH,\"total-comments.pkl\"))\n",
    "total_comments_df = pd.read_pickle(os.path.join(DATA_PATH,\"total-comments.pkl\"))\n",
    "total_comments_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize comments\n",
    "\n",
    "- 추출 태그 = [\"Noun\", \"Adjective\"]\n",
    "- stopwords = [\"것\", \"이\", \"안\", \"더\", \"왜\", \"때\", \"좀\", \"뭐\", \"거\", \"저\", \"뿐\", \"머\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T09:46:45.743724Z",
     "start_time": "2020-03-21T07:06:49.806561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2393070/2393070 [2:39:55<00:00, 249.38it/s]\n"
     ]
    }
   ],
   "source": [
    "token_data = []\n",
    "\n",
    "for comment in tqdm.tqdm(total_comments_df.comment):\n",
    "    token = tokenize_comment(comment)\n",
    "    token_data.append(token)                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T09:47:12.484224Z",
     "start_time": "2020-03-21T09:46:53.900912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "있다(332358) 되다(276578) 없다(251559) 아니다(235439) 보다(233705) 사람(183788) 같다(148385) 않다(133126) 그렇다(128083) 들다(127041) 가다(118254) 좋다(113548) 말(111371) 받다(106700) 놈(102425) 돈(98007) 먹다(97399) 왜(96503) 국민(95629) 더(92500) 한(88661) 생각(87983) 때(87228) 뭐(84020) 좀(83061) 나라(82380) 나오다(81764) 많다(80165) 개(77136) 이렇다(76579) 알다(75282) 시키다(74477) 거(71876) 내(66978) 살다(66441) 모르다(64624) 우리(62604) 차다(62285) 만들다(62027) 인간(61788) 한국(61260) 자다(61078) 기사(60117) 너무(59581) 지다(58863) 진짜(58431) 안되다(58150) 일(58139) 주다(57874) 니(56490) \n",
      "Total Vocab:  193373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys, n_vocab = count_comment(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T09:47:21.530317Z",
     "start_time": "2020-03-21T09:47:20.886757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [이국, 종, 교수, 외과, 야전, 사령관, 분, 업무, 차질, 없다, 물, 심양,...\n",
       "1          [문재인, 대통령, 이국, 종, 교수, 귀순, 병사, 모두, 건강하다, 비다]\n",
       "2                    [단, 한번, 써다, 보지, 포인트, 절반, 통신비, 차감]\n",
       "3                        [우리나라, 통신, 업체, 차다, 쉬다, 돈, 벌다]\n",
       "4                                     [진짜, 쓸다, 너무, 적다]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_token_data = pd.Series(token_data)\n",
    "series_token_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T09:47:30.142949Z",
     "start_time": "2020-03-21T09:47:29.886637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2393065</th>\n",
       "      <td>20170731044235632</td>\n",
       "      <td>6D6963D736FF40328D170B475A4F34F0</td>\n",
       "      <td>인천에 자유 뭐시기 소속 구청장이 좀 있는데 어디냐 서구냐 남구냐 연수구냐 남동구냐...</td>\n",
       "      <td>[인천, 자유, 뭐시기, 소속, 구청, 좀, 있다, 어디, 서구, 남구, 연수구, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393066</th>\n",
       "      <td>20170731044235632</td>\n",
       "      <td>0FB426A0CFF2443D92E8D96402008692</td>\n",
       "      <td>맹바기닮은것보니 자유당일듯...</td>\n",
       "      <td>[맹바기, 닮다, 보다, 자유당]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393067</th>\n",
       "      <td>20170731044235632</td>\n",
       "      <td>897F2E5AC2D843628B24244DF4FF77BB</td>\n",
       "      <td>멍박그네  정권 싸 똥치우는  문정부 개고생. 적폐청산 MB 방산 자원외교 4대강 ...</td>\n",
       "      <td>[멍박, 그네, 정권, 싸다, 똥, 치우다, 정부, 개, 고생, 적폐, 청산, 방산...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393068</th>\n",
       "      <td>20170731044235632</td>\n",
       "      <td>80A31650BF054D2AAE92310374373DB9</td>\n",
       "      <td>이제 명박이좀 잊을때도 안됐나 그만하고 새출발하자 듣는것도 지겹다</td>\n",
       "      <td>[이제, 명박, 좀, 잊다, 때, 돼다, 그만하다, 출발, 하자, 듣다, 지겹다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393069</th>\n",
       "      <td>20170731044235632</td>\n",
       "      <td>B9643D95F2F045A9808C2CAA0B70F25B</td>\n",
       "      <td>C구청장을 당장 구속 수사하면 될듯~~~</td>\n",
       "      <td>[청장, 당장, 구속, 수사, 되다]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   news_id                        comment_id  \\\n",
       "2393065  20170731044235632  6D6963D736FF40328D170B475A4F34F0   \n",
       "2393066  20170731044235632  0FB426A0CFF2443D92E8D96402008692   \n",
       "2393067  20170731044235632  897F2E5AC2D843628B24244DF4FF77BB   \n",
       "2393068  20170731044235632  80A31650BF054D2AAE92310374373DB9   \n",
       "2393069  20170731044235632  B9643D95F2F045A9808C2CAA0B70F25B   \n",
       "\n",
       "                                                   comment  \\\n",
       "2393065  인천에 자유 뭐시기 소속 구청장이 좀 있는데 어디냐 서구냐 남구냐 연수구냐 남동구냐...   \n",
       "2393066                                  맹바기닮은것보니 자유당일듯...   \n",
       "2393067  멍박그네  정권 싸 똥치우는  문정부 개고생. 적폐청산 MB 방산 자원외교 4대강 ...   \n",
       "2393068               이제 명박이좀 잊을때도 안됐나 그만하고 새출발하자 듣는것도 지겹다   \n",
       "2393069                             C구청장을 당장 구속 수사하면 될듯~~~   \n",
       "\n",
       "                                             comment_token  \n",
       "2393065  [인천, 자유, 뭐시기, 소속, 구청, 좀, 있다, 어디, 서구, 남구, 연수구, ...  \n",
       "2393066                                 [맹바기, 닮다, 보다, 자유당]  \n",
       "2393067  [멍박, 그네, 정권, 싸다, 똥, 치우다, 정부, 개, 고생, 적폐, 청산, 방산...  \n",
       "2393068      [이제, 명박, 좀, 잊다, 때, 돼다, 그만하다, 출발, 하자, 듣다, 지겹다]  \n",
       "2393069                               [청장, 당장, 구속, 수사, 되다]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_comments_df = total_comments_df.reset_index(drop=True)\n",
    "total_comments_df['comment_token'] = series_token_data\n",
    "total_comments_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T09:47:55.238877Z",
     "start_time": "2020-03-21T09:47:38.472679Z"
    }
   },
   "outputs": [],
   "source": [
    "total_comments_df.to_pickle(\"dataset/total-comments_with_token.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:40:15.358930Z",
     "start_time": "2020-03-26T09:40:00.985365Z"
    }
   },
   "outputs": [],
   "source": [
    "token_data = pd.read_pickle(\"dataset/total-comments_with_token.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:40:15.410791Z",
     "start_time": "2020-03-26T09:40:15.390846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20171201145847479</td>\n",
       "      <td>F3C2DFCFE9E947B19C8A3053DD5C821B</td>\n",
       "      <td>이국종교수는  외과 야전사령관 이다.  그분의 업무에 차질 없도록 물심양면 으로  ...</td>\n",
       "      <td>[이국, 종, 교수, 외과, 야전, 사령관, 분, 업무, 차질, 없다, 물, 심양,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20171201145847479</td>\n",
       "      <td>61A3B6AAD6124DFFB70EC4275496F8A6</td>\n",
       "      <td>문재인 대통령님과 이국종 교수님, 그리고 귀순병사 모두 건강하시길 빕니다.</td>\n",
       "      <td>[문재인, 대통령, 이국, 종, 교수, 귀순, 병사, 모두, 건강하다, 비다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20171201060244786</td>\n",
       "      <td>B6660EF2C7C14A2F903A08363641CF09</td>\n",
       "      <td>단 한번도 써보지 못했다ㆍ 포인트의 절반만이라도 통신비로 차감을!</td>\n",
       "      <td>[단, 한번, 써다, 보지, 포인트, 절반, 통신비, 차감]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20171201060244786</td>\n",
       "      <td>C4F129BFB1FD4543A24A03D96F38351D</td>\n",
       "      <td>우리나라 통신업체들  참 쉽게  돈 벌어요</td>\n",
       "      <td>[우리나라, 통신, 업체, 차다, 쉬다, 돈, 벌다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20171201060244786</td>\n",
       "      <td>1A21FDDF151E45DF808F0E5B662E8F3C</td>\n",
       "      <td>진짜 쓸데가너무적다.</td>\n",
       "      <td>[진짜, 쓸다, 너무, 적다]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             news_id                        comment_id  \\\n",
       "0  20171201145847479  F3C2DFCFE9E947B19C8A3053DD5C821B   \n",
       "1  20171201145847479  61A3B6AAD6124DFFB70EC4275496F8A6   \n",
       "2  20171201060244786  B6660EF2C7C14A2F903A08363641CF09   \n",
       "3  20171201060244786  C4F129BFB1FD4543A24A03D96F38351D   \n",
       "4  20171201060244786  1A21FDDF151E45DF808F0E5B662E8F3C   \n",
       "\n",
       "                                             comment  \\\n",
       "0  이국종교수는  외과 야전사령관 이다.  그분의 업무에 차질 없도록 물심양면 으로  ...   \n",
       "1          문재인 대통령님과 이국종 교수님, 그리고 귀순병사 모두 건강하시길 빕니다.   \n",
       "2               단 한번도 써보지 못했다ㆍ 포인트의 절반만이라도 통신비로 차감을!   \n",
       "3                            우리나라 통신업체들  참 쉽게  돈 벌어요   \n",
       "4                                        진짜 쓸데가너무적다.   \n",
       "\n",
       "                                       comment_token  \n",
       "0  [이국, 종, 교수, 외과, 야전, 사령관, 분, 업무, 차질, 없다, 물, 심양,...  \n",
       "1        [문재인, 대통령, 이국, 종, 교수, 귀순, 병사, 모두, 건강하다, 비다]  \n",
       "2                  [단, 한번, 써다, 보지, 포인트, 절반, 통신비, 차감]  \n",
       "3                      [우리나라, 통신, 업체, 차다, 쉬다, 돈, 벌다]  \n",
       "4                                   [진짜, 쓸다, 너무, 적다]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:40:15.455670Z",
     "start_time": "2020-03-26T09:40:15.439714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [이국, 종, 교수, 외과, 야전, 사령관, 분, 업무, 차질, 없다, 물, 심양,...\n",
       "1          [문재인, 대통령, 이국, 종, 교수, 귀순, 병사, 모두, 건강하다, 비다]\n",
       "2                    [단, 한번, 써다, 보지, 포인트, 절반, 통신비, 차감]\n",
       "3                        [우리나라, 통신, 업체, 차다, 쉬다, 돈, 벌다]\n",
       "4                                     [진짜, 쓸다, 너무, 적다]\n",
       "Name: comment_token, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = token_data['comment_token']\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding \n",
    "\n",
    "두 모델 학습해서 이후 결과 비교해볼 것 \n",
    " - tokenized_comments\n",
    "     - word2vec\n",
    "     - fastText\n",
    " - 자모 단위 분해 + fastText\n",
    " \n",
    "**Reference**    \n",
    "- [Finding best fasttext hyperparameters](http://soner.in/fasttext-grid-search/)\n",
    "- [한국어 단어 임베딩을 위한 Word2vec 모델의 최적화](http://journal.dcs.or.kr/xml/19540/19540.pdf)\n",
    "- [FastText, Word representation using subword](https://lovit.github.io/nlp/representation/2018/10/22/fasttext_subword/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:40:19.161791Z",
     "start_time": "2020-03-26T09:40:15.480605Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:40:19.202653Z",
     "start_time": "2020-03-26T09:40:19.192680Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "config = {\n",
    "    'min_count': 1,  # 등장 횟수가 1 이하인 단어는 무시\n",
    "    'size': 300,  # 300차원짜리 벡터스페이스에 embedding\n",
    "    'sg': 1,  # 0이면 CBOW, 1이면 skip-gram을 사용한다\n",
    "    'batch_words': 10000,  # 사전을 구축할때 한번에 읽을 단어 수\n",
    "    'iter': 100,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "    'workers': multiprocessing.cpu_count(),\n",
    "    'window': 5,\n",
    "    'seed': 25 #random number,\n",
    "}\n",
    "##model =  word2vec.Word2Vec(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T11:29:16.625510Z",
     "start_time": "2020-03-26T09:40:20.912082Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model_w2v = word2vec.Word2Vec(tokens,**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:21:59.166963Z",
     "start_time": "2020-03-27T08:21:55.547613Z"
    }
   },
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "model_w2v.save(os.path.join(DATA_PATH,'embedding',\"w2v_0327.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:23:03.681360Z",
     "start_time": "2020-03-27T08:23:02.931363Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "w2v_model = word2vec.Word2Vec.load(os.path.join(DATA_PATH,'embedding',\"w2v_0327.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analogy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:23:20.547237Z",
     "start_time": "2020-03-27T08:23:20.131349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짱깨', 0.7469527721405029),\n",
       " ('짱개', 0.7402806878089905),\n",
       " ('중공', 0.7356770038604736),\n",
       " ('짱꼴라', 0.6442628502845764),\n",
       " ('러시아', 0.6201988458633423),\n",
       " ('중국인', 0.6164897084236145),\n",
       " ('중궈', 0.5840282440185547),\n",
       " ('시진핑', 0.5833264589309692),\n",
       " ('일본', 0.560553789138794),\n",
       " ('미국', 0.5600796937942505)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = \"중국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:23:24.636304Z",
     "start_time": "2020-03-27T08:23:24.619371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('쪽바리', 0.637258768081665),\n",
       " ('일본도', 0.6027649641036987),\n",
       " ('롯게', 0.5917129516601562),\n",
       " ('좃선님', 0.5848301649093628),\n",
       " ('일본인', 0.5770894289016724),\n",
       " ('쪽발이', 0.5697472095489502),\n",
       " ('중국', 0.560553789138794),\n",
       " ('왜놈', 0.5594272613525391),\n",
       " ('넘듬', 0.5572184324264526),\n",
       " ('짓맓고', 0.5498552918434143)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = \"일본\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:23:27.532558Z",
     "start_time": "2020-03-27T08:23:27.515605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대통', 0.7586009502410889),\n",
       " ('문재인', 0.6966331005096436),\n",
       " ('통령', 0.6032859086990356),\n",
       " ('영부인', 0.6013497710227966),\n",
       " ('총리', 0.5542508363723755),\n",
       " ('망녕났군', 0.5260519981384277),\n",
       " ('박성욱', 0.5248264670372009),\n",
       " ('김정숙', 0.5233049392700195),\n",
       " ('땔래야땔수없', 0.5201804041862488),\n",
       " ('부럽쥬', 0.5174517631530762)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = \"대통령\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText 모델 학습 및 불러오기\n",
    "\n",
    "- 학습 데이터와 파라미터 동일하게 해서 이후에 word2vec과 결과 비교\n",
    "- 자모 단위 모델 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:26:20.392515Z",
     "start_time": "2020-03-27T08:26:20.387551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이국', '종', '교수', '외과', '야전', '사령관', '분', '업무', '차질', '없다', '물', '심양', '도', '주기', '분', '역활']\n",
      "2393070\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T08:28:28.590910Z",
     "start_time": "2020-03-27T08:28:28.587892Z"
    }
   },
   "outputs": [],
   "source": [
    "ft_config = {\n",
    "    'min_count': 1,  # 등장 횟수가 1 이하인 단어는 무시\n",
    "    'size': 300,  # 300차원짜리 벡터스페이스에 embedding\n",
    "    'sg': 1,  # 0이면 CBOW, 1이면 skip-gram을 사용한다\n",
    "    'batch_words': 10000,  # 사전을 구축할때 한번에 읽을 단어 수\n",
    "    'iter': 100,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "    'workers': multiprocessing.cpu_count(),\n",
    "    'window': 5,\n",
    "    'seed': 25, #random number,\n",
    "    'word_ngrams':1 # uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec.\n",
    "}\n",
    "\n",
    "model2 = FastText(size=4, window=3, min_count=1, sentences=common_texts, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T09:28:14.128615Z",
     "start_time": "2020-03-27T08:34:39.477928Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e8ecc666097a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfastText_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mft_config\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "fastText_model = fasttext.FastText(sentences=tokens, **ft_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연관성 테스트\n",
    "fastText_model.get_nearest_neighbors(\"문재인\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "#fastText_model.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "#fastText_model = fasttext.load_model(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
